{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec73ea11",
   "metadata": {},
   "source": [
    "### https://techblog-history-younghunjo1.tistory.com/376\n",
    "- 신경망 모델은 덧셈, 곱셈 연산도 존재하지만 가장 핵심은 Activation Function Operation 이다.\n",
    "- 활성화 함수 종류에 따라 오차역전파 방법이 차이가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ebe0e",
   "metadata": {},
   "source": [
    "# 1. ReLU\n",
    "- ReLU는 x가 0보다 작거나 같을 때, y를 모두 0으로 차단해버리는 활성화함수 이다.\n",
    "- ReLU 함수의 x에 대한 y의 미분은 순전파 때의 입력 데이터가 0보다 컸다면, 이를 역전파 수행할 때 이전 노드에서 흘러들어온 국소적인 미분 값을 그대로 흘려보내는 것을 의미한다.\n",
    "- 반대로 순전파 시 x가 0이었다면 이를 역전파 수행 시, 이전 노드에서 흘러들어온 국소적인 미분 값을 전달하지 않는 것을 의미한다. x가 0 이하일 때 미분값이 0이기 때문에 이전 노드에서 어떤 값이 흘러나왔던 간에 0을 곱하면 0이 되기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd2c95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input - [2. 0. 2. 0. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0.]\n",
      "Forward Propagation - [2. 0. 2. 0. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0.]\n",
      "Backward Propagation - [2. 0. 2. 0. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Relu 활성함수의 역전파 계층 만들기\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x: np.array):\n",
    "        # 0보다 작거나 같은 값은 출력값 0처리,\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_out[self.mask] = 0\n",
    "        dx = d_out\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "datas = np.append(np.zeros(10), np.ones(10) + 1)\n",
    "np.random.shuffle(datas)\n",
    "print(\"Input - {}\".format(datas))\n",
    "\n",
    "relu_layer = Relu()\n",
    "forward_result = relu_layer.forward(datas)\n",
    "print(\"Forward Propagation - {}\".format(forward_result))\n",
    "\n",
    "back_result = relu_layer.backward(datas)\n",
    "print(\"Backward Propagation - {}\".format(back_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2337eb8",
   "metadata": {},
   "source": [
    "# 2. Sigmoid\n",
    "- 시그모이드 함수 연산은 계산 그래프(Computation Graph)로 펼쳐서 확인할 수 있다.\n",
    "1. 나누기 연산\n",
    "2. 덧셈 연산\n",
    "3. exp(Exponential) 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9620a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input - [2. 0. 2. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 0. 0.]\n",
      "Forward Propagation - [0.88079708 0.5        0.88079708 0.5        0.5        0.5\n",
      " 0.88079708 0.88079708 0.5        0.88079708 0.5        0.88079708\n",
      " 0.88079708 0.88079708 0.88079708 0.5        0.88079708 0.5\n",
      " 0.5        0.5       ]\n",
      "Backward Propagation - [0.20998717 0.         0.20998717 0.         0.         0.\n",
      " 0.20998717 0.20998717 0.         0.20998717 0.         0.20998717\n",
      " 0.20998717 0.20998717 0.20998717 0.         0.20998717 0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x: np.array):\n",
    "        # sigmoid\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        return dx\n",
    "    \n",
    "datas = np.append(np.zeros(10), np.ones(10) + 1)\n",
    "np.random.shuffle(datas)\n",
    "print(\"Input - {}\".format(datas))\n",
    "\n",
    "sig_layer = Sigmoid()\n",
    "forward_result = sig_layer.forward(datas)\n",
    "print(\"Forward Propagation - {}\".format(forward_result))\n",
    "\n",
    "back_result = sig_layer.backward(datas)\n",
    "print(\"Backward Propagation - {}\".format(back_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a9457",
   "metadata": {},
   "source": [
    "# 3. Affine\n",
    "- Affine은 행렬의 곱을 의미한다.\n",
    "    - 신경망은 Forward Propagation을 수행할 때, 행렬의 곱을 수행한다. X * W + b\n",
    "    - Affine은 기하학에서 행렬의 곱을 의마한다고 해서, 행렬의 곱 연산에 대한 Forward Propagation 및 Back Propagation을 수행하는 계층을 Affine 계층이라고 부른다.\n",
    "- Back propagation은 파라미터들의 변화량을 구함으로써, 기존 파라미터 값에 더해주거나 빼주기 위함이 목적이다. 그렇기 때문에 Back propagation의 결과로 얻은 각 파라미터의 변화량이 담긴 행렬의 shape가 파라미터의 shape와 동일해야 한다.\n",
    "    - (3,) X (?, ?) = (2,)\n",
    "    - -> (3,) X (3, 2) = (2,)\n",
    "    - (?, ?) X (1, 3) = (2, 3)\n",
    "    - -> (2, 1) X (1, 3) = (2, 3)\n",
    "- 편향(Bias)의 고려 : 편향인 b의 미분값을 N개의 데이터마다 더해서 구하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "318572e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.matmul(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        dx = np.dot(d_out, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, d_out)\n",
    "        self.db = np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1415082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [0.41753093 0.77283542]\n",
      "Weights [[ 1.46118191 -0.92181319  0.59844233]\n",
      " [ 0.15405787 -0.92417    -0.17063844]]\n",
      "Bias [-1.55900431 -1.43636483  0.71649845]\n",
      "------------------------\n",
      "Output [-0.82985429 -2.53548166  0.8344912 ]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size=(2,))\n",
    "W = np.random.normal(size=(2,3))\n",
    "b = np.random.normal(size=(3,))\n",
    "\n",
    "print(\"Input\", X)\n",
    "print(\"Weights\", W)\n",
    "print(\"Bias\", b)\n",
    "print(\"------------------------\")\n",
    "\n",
    "affine_layer = Affine(W, b)\n",
    "out = affine_layer.forward(X)\n",
    "print(\"Output\",out)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# dx = affine_layer.backward(out)\n",
    "# print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5fdae",
   "metadata": {},
   "source": [
    "# 4. Softmax-with-Loss\n",
    "- 여기서의 Loss는 Cross Entropy를 의미\n",
    "- Softmax + Cross Entropy 함수가 함께 있는 계층\n",
    "- Softmax 계층은 보통 신경망 모델을 학습시킬 때만 사용하고, 테스트 데이터에 대한 예측! 즉, '추론'하는 단계에서는 보통 사용하지 않는다.\n",
    "    - softmax 계층은 예측 출력값과 실제 정답 간의 차이(손실 함수)를 0으로 만드는 파라미터 변화량을 찾는 즉, 학습할 때에 기여하기 때문이다.\n",
    "    - Softmax 함수는 파라미터 최적화에 크게 기여\n",
    "    - '추론' 단계에서는 단순히 출력값이 큰 것을 최종 출력으로 내뱉기만 하면 되므로, 손실함수를 건드릴 필요가 없다.\n",
    "- Softmax with Loss는 계산 그래프의 구조는 복잡하지만, 최종적인 미분값이 y라는 예측값과 t라는 정답만을 갖고 계산할 수 있다라는 특징을 가진다. 손실함수로 Cross Entropy를 사용하기 때문이라고 한다. CEE가 그렇게 설계되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee20bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # Prediction\n",
    "        self.t = None # label\n",
    "        \n",
    "    def softmax(x):\n",
    "        max_x = np.max(x)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        exp_X_sum = np.sum(exp_x)\n",
    "        \n",
    "        return exp_x / exp_x_sum\n",
    "    \n",
    "    def cross_entropy_error(y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        \n",
    "        # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "            \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
