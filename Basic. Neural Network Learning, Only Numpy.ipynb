{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bce44e",
   "metadata": {},
   "source": [
    "### https://techblog-history-younghunjo1.tistory.com/372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6648487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c138bb4",
   "metadata": {},
   "source": [
    "# 1. Loss Function\n",
    "- 손실함수란 \"파라미터를 어떤 방향으로 학습시킬지에 대한 가이드라인\"\n",
    "- 신경망 학습 프로세스\n",
    "    1. 어떤 것을 가지고 학습? 데이터\n",
    "    2. 무엇을 학습? 가중치, 편향\n",
    "    3. 어떤 기준으로 학습? 손실 함수 값이 0이 되도록 하습\n",
    "    4. 0이 되도록 어떻게? 손실 함수의 기울기를 활용해 가중치, 편향의 변화량 얻기\n",
    "    5. 어떻게 기울기를 계산?\n",
    "        a. 수치 미분을 통해 계산 (계산이 쉽지만 속도 느림)\n",
    "        b. 오차 역전파 통해 계산 (계산이 복잡하지만 빠름)\n",
    "    6. 얻은 변화량 만큼 가중치, 편향 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8929bffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19500000000000006"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 오차제곱합 (Sum of Squares for Error, SSE)\n",
    "def sse(y, y_pred):\n",
    "    loss = np.sum((y_pred - y) ** 2)\n",
    "    return loss\n",
    "\n",
    "y = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y_pred = np.array([0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0])\n",
    "loss = sse(y, y_pred)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70cf03af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.         -0.51082546 -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 교차 엔트로피 오차 (Cross-Entropy Error, CEE)\n",
    "def cross_entropy(y, y_pred):\n",
    "    delta = 1e-7\n",
    "    print(y * np.log(y_pred + delta))\n",
    "    loss = -np.sum(y * np.log(y_pred + delta))\n",
    "    \n",
    "    return loss\n",
    "loss = cross_entropy(y, y_pred)\n",
    "loss\n",
    "\n",
    "## 현재 데이터\n",
    "### y : 0과 1로 이루어진 one-hot vector 형태의 데이터\n",
    "### y_pred : softmax의 확률 예측 값 형태\n",
    "### 교차 엔트로피 오차는 실제 값 y가 1일 때만 계산이 들어가는 형태이며, 자연로그를 계산하게 된다.\n",
    "#### -> 오른쪽 수식의 값이 작을수록 교차 엔트로피 오차는 작다는 것을 의미한다.\n",
    "#### 해당에서 소개한 수식은 데이터 1개에 대한 수식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fc43dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 교차 엔트로피 오차, 미니배치를 위한\n",
    "def cross_entropy_ohe(y, y_pred):\n",
    "    # 데이터가 한개만 들어왔을 경우 2차원의 데이터로 변환\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(1, y_pred.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    # 한 배치당 데이터 갯수\n",
    "    batch_size = y.shape[0]\n",
    "    # batch_size를 나누어주는 정규화(normalization) 작업 추가\n",
    "    loss = -np.sum(y * np.log(y_pred + 1e-7)) / batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5c5af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 교차 엔트로피 오차, 미니배치를 위한, 이제 레이블 인코딩 된\n",
    "def cross_entropy(y, y_pred):\n",
    "    # 데이터가 한개만 들어왔을 경우 2차원의 데이터로 변환\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(1, y_pred.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    # 한 배치당 데이터 갯수\n",
    "    batch_size = y.shape[0]\n",
    "    # batch_size를 나누어주는 정규화(normalization) 작업 추가\n",
    "    loss = -np.sum(np.log(y_pred[np.arange(batch_size), y] + 1e-7)) / batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5051b9",
   "metadata": {},
   "source": [
    "### 손실함수의 의미\n",
    "- 정확도(Accuracy)와 같은 미분이 하는 역할의 수식은 불연속적인 값을 내뱉는 경향이 강하기 때문에 적절하지 못하다.\n",
    "- 손실함수는 입력값을 받아 활성함수를 결과값을 기반으로 실행이 되어지는데, 활성함수를 이산적인 값들만 내뱉게 되는 계단함수를 사용하게 되면, 손실함수도 미분하게 적절한 모양으로 나오지 않을 것이 분명하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549b01c",
   "metadata": {},
   "source": [
    "# 2. 수치 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "059a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변화량 값을 계산하기 위해 활용되는 수치미분\n",
    "# 이제 중심(중앙) 차분으로의 변형 형태 (h를 두번 사용)\n",
    "def numerical_diff(f, x):\n",
    "    # 거의 고정값\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182c2fe",
   "metadata": {},
   "source": [
    "# 3. 편미분\n",
    "- 수치 미분은 1개의 변수에 대한 미분을 의미했다면, 편미분은 변수가 2개 이상일 때의 미분을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aaabe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 2차 함수로 구성된 새로운 함수 정의\n",
    "def function_2(x: np.array):\n",
    "    return np.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4dd60c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n"
     ]
    }
   ],
   "source": [
    "# 2. 1의 함수를 편미분\n",
    "# 한 변수씩 미분을 차례대로 수행해준다.\n",
    "# 이 때, x1값은 상수로 고정한다.\n",
    "def square_func_temp1(x0):\n",
    "    y = x0 * x0 + 4.0 ** 2 # x1을 4.0으로 고정한 형태\n",
    "    return y\n",
    "\n",
    "# 아래의 수식의 의미\n",
    "# x1을 상수로 고정시킨 후 x0가 3일 때, h(1e-4)만큼 늘리면 f값은 얼마나 변화했는가를 의미\n",
    "print(numerical_diff(square_func_temp1, 3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46170511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "# x0을 상수로 고정시키고 x1에 대한 편미분 수행\n",
    "def square_func_temp2(x1):\n",
    "    y = 3.0 ** 2 + x1 * x1 # x0을 3.0으로 고정한 형태\n",
    "    return y\n",
    "\n",
    "print(numerical_diff(square_func_temp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e8a34",
   "metadata": {},
   "source": [
    "# 4. 기울기\n",
    "- 모든 변수의 편미분을 벡터(행렬)로 정리한 것을 바로 기울기(Gradient)라고 한다.\n",
    "- 즉, 여러 변수들의 변화량들을 모아놓은 것을 이야기 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "650c9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 0.]\n",
      "[16.  0.]\n",
      "[-10.   0.]\n"
     ]
    }
   ],
   "source": [
    "def function_2(x: np.array):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def numerical_gradient(f, x:np.array):\n",
    "    h = 1e-4\n",
    "    \n",
    "    # 기울기를 담아놓은 벡터 행렬 초기화\n",
    "    gradients = np.zeros_like(x)\n",
    "    \n",
    "    # array 원소 하나씩 편미분 수행\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x*h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f값 계산을 위해서는 모든 x들을 필요로함\n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        # 미분 공식 수행\n",
    "        gradients[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val # 다른 변수의 편미분을 수행해주기 위해서 -h/+h 했던 원소값을 복원\n",
    "        return gradients\n",
    "    \n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([8.0, 10.0])))\n",
    "print(numerical_gradient(function_2, np.array([-5.0, -3.0])))\n",
    "\n",
    "# 기울기 값의 의미\n",
    "# 기울기의 변화량 값이 음수이면, 가중치와 편향 값들을 양의 방향으로 변화시켜야 함을 의미하고,\n",
    "# 변화량 값이 양수라면, 가중치와 편향 값들을 음의 방향으로 변화시켜야 함을 의미한다.\n",
    "# 변화량들의 집합인 기울기는 일종의 '벡터' 이다. 이것들은 대소비교의 의미가 아닌, '방향' 성을 가리킴을 인지하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75ddb9",
   "metadata": {},
   "source": [
    "# 5. 경사하강법 (Gradient Descent)\n",
    "- 기울기(gradients)로 각 가중치와 편향 값들을 갱신해주는 방법\n",
    "- 경사하강법이란 기울기 값이라는 방향성을 가이드로 삼아 손실함수 값의 최솟값 또는 최댓값을 찾으려는 것을 이야기 한다.\n",
    "- 하지만 기울기가 가리키는 방향으로 간다고 해서 무조건 손실함수 값을 최솟값 또는 최댓값으로 간다는 보장은 없다. 이럴 때, 우리가 찾은 값이 극솟값(Local Minimum)인지 최솟값(Global Minimum)인지 모르다고 이야기 한다.\n",
    "- 하이퍼 파라미터로 학습률(Learning rate)을 받는데, 이는 파라미터 갱신 시 얼마만큼 갱신시켜야 할지를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ff3f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.10953066e-10  4.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 변수가 2개일 때의 경사하강법\n",
    "def function_2(x: np.array):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def gradient_descent(f, init_x: np.array, lr=0.01, step_num=100):\n",
    "    x = init_x # 100번의 경사하강 수행\n",
    "    for _ in range(step_num):\n",
    "        # 1. 손실 함수인 function_2를 기반으로 기울기를 반복 계산\n",
    "        gradients = numerical_gradient(f, x)\n",
    "        # 2. x값 업데이트\n",
    "        x -= lr * gradients\n",
    "    return x\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=0.1, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16b011",
   "metadata": {},
   "source": [
    "# 6. Learning Algorithm, Two Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f849eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function: Sigmoid, Softmax\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f230d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f90472dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient function\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "#         print(fxh1)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "#         print(fxh2)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "#         print(grad[idx])\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae60893",
   "metadata": {},
   "source": [
    "## 1. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c1a25",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f5d7f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data \n",
    "from datas.mnist import load_mnist\n",
    "\n",
    "# datas\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=False)\n",
    "\n",
    "# history datas\n",
    "train_loss = []\n",
    "\n",
    "# Hyper Parameter\n",
    "epochs = 10\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa6a2d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0cf0c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params. Weights, Bias Setting\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "output_size=10\n",
    "\n",
    "params = {}\n",
    "params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "params['b1'] = np.zeros(hidden_size,)\n",
    "params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "params['b2'] = np.zeros(output_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3d3d5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "batch_idx = np.random.choice(train_size, batch_size)\n",
    "\n",
    "X_batch = X_train[batch_idx]\n",
    "y_batch = y_train[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f5e79b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict():\n",
    "    layer_size = int(len(params.keys()) / 2)\n",
    "\n",
    "    w1, w2 = [params['W{}'.format(_ + 1)] for _ in range(layer_size)]\n",
    "    b1, b2 = [params['b{}'.format(_ + 1)] for _ in range(layer_size)]\n",
    "\n",
    "    # Layer 1\n",
    "    output = np.matmul(X_batch, w1) + b1\n",
    "    output = sigmoid(output)\n",
    "    # Layer 2\n",
    "    output = np.matmul(output, w2) + b2\n",
    "    output = softmax(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bce1572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before gradients, loss value : 7.7230217886787\n"
     ]
    }
   ],
   "source": [
    "# 기울기를 위한 손실함수 생성\n",
    "def loss_func(W):\n",
    "    y_pred = predict()\n",
    "    return cross_entropy_error(y_pred, y_batch)\n",
    "\n",
    "loss_value = loss_func(None)\n",
    "print(\"before gradients, loss value : {}\".format(loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0be2d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 계산\n",
    "gradients = dict()\n",
    "\n",
    "for key in params.keys():\n",
    "    gradients[key] = numerical_gradient(loss_func, params[key])\n",
    "    \n",
    "# 위에 작성한 손실함수를 변경 시켜준 것을 확인할 수 있다.\n",
    "# 손실함수에서 사용하는 글로벌 변수들의 연산으로 나오는 y_pred와 y_batch는 고정적인 글로벌 값이 되는데 계속 같은 값의 비교만 진행하면\n",
    "# 기울기가 나타나지 않지 않나?\n",
    "\n",
    "# 핵심은 params[key]에 있다. 가중치 혹은 편향을 매개변수로 주는데 이 때의 호출 방식은 call by reference 이다.\n",
    "# 즉, numeric_gradient에서 다루고 있는 것들은 전역 가중치 혹은 편향이다.\n",
    "# 하나씩 변경해 가면서 predict()를 해주는데 이 때 변화를 주는 가중치 혹은 편향의 값이 있을 것 이다.\n",
    "# 이에 따라 변화량을 기록한다. 그 후, 다음 아이템의 연산을 위하여 원본값으로 변경시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "302b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습율에 따른 기울기 적용\n",
    "for key in params.keys():\n",
    "    params[key] -= learning_rate * gradients[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b1152fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after gradients, loss value : 7.2064644383203005\n"
     ]
    }
   ],
   "source": [
    "# 줄어든다,, 대박,,\n",
    "loss_value = loss_func(None)\n",
    "print(\"after gradients, loss value : {}\".format(loss_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
