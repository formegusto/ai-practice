{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883fe93c",
   "metadata": {},
   "source": [
    "#### https://techblog-history-younghunjo1.tistory.com/382"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa73d2",
   "metadata": {},
   "source": [
    "# 1. Activaion Funcion, Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c74480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. activation func - sigmoid\n",
    "def sigmoid(x: np.array):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 2. activation func - relu\n",
    "def relu(x: np.array):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# 3. activation func - softmax \n",
    "def softmax(x: np.array):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x -= np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    \n",
    "    x -= np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03441572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. loss func - cross-entropy-error\n",
    "def cross_entropy_error(y: np.array, t: np.array):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    # one-hot vector type\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aee2b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5, 수치 미분 계산 함수\n",
    "def numerical_gradient(f, x: np.array):\n",
    "    h = 1e-4\n",
    "    grads = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        # f(x + h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        # f(x - h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grads[idx] = (fx1 - fx2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf3ba5",
   "metadata": {},
   "source": [
    "# 2. 활성화 함수(Relu, Sigmoid) 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e420036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Relu Layer\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x:np.array):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "# 2. Sigmoid Layer\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        y = sigmoid(x)\n",
    "        self.y = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y * (1 - self.y)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cc09d",
   "metadata": {},
   "source": [
    "# 3. 행렬 곱(Affine) 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7453aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Affine Layer\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        y = np.matmul(self.x, self.W) + self.b\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.matmul(dout, self.W.T)\n",
    "        self.dW = np.matmul(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b5fe5",
   "metadata": {},
   "source": [
    "# 4. Softmax, Loss(Cross Entrophy Error) 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8c87862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x: np.array, t: np.array):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47649d6c",
   "metadata": {},
   "source": [
    "# 5. 2-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78f9991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 2층 신경망의 파라미터 딕셔너리\n",
    "        self.params = dict()\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 2층 신경망의 계층 생성 \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = self.lastlayer.forward(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "            \n",
    "        acc = np.sum(y == t) / float(y.shape[0])\n",
    "        return acc\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # Forward propagation\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # Back propagation - 1. Softmax-with-Loss Layer\n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        # Back propagation - 2. Rest Layer\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # Back propagation result\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 기울기 계산\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_w, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_w, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebfd147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datas.mnist import load_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b2921c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 - train accuracy:0.11 - test accuracy:0.092\n",
      "epochs 11 - train accuracy:0.18 - test accuracy:0.166\n",
      "epochs 21 - train accuracy:0.26 - test accuracy:0.254\n",
      "epochs 31 - train accuracy:0.39 - test accuracy:0.302\n",
      "epochs 41 - train accuracy:0.32 - test accuracy:0.292\n",
      "epochs 51 - train accuracy:0.41 - test accuracy:0.417\n",
      "epochs 61 - train accuracy:0.44 - test accuracy:0.424\n",
      "epochs 71 - train accuracy:0.66 - test accuracy:0.572\n",
      "epochs 81 - train accuracy:0.68 - test accuracy:0.616\n",
      "epochs 91 - train accuracy:0.68 - test accuracy:0.697\n",
      "epochs 101 - train accuracy:0.83 - test accuracy:0.758\n",
      "epochs 111 - train accuracy:0.78 - test accuracy:0.773\n",
      "epochs 121 - train accuracy:0.75 - test accuracy:0.783\n",
      "epochs 131 - train accuracy:0.76 - test accuracy:0.8\n",
      "epochs 141 - train accuracy:0.85 - test accuracy:0.813\n",
      "epochs 151 - train accuracy:0.81 - test accuracy:0.824\n",
      "epochs 161 - train accuracy:0.79 - test accuracy:0.823\n",
      "epochs 171 - train accuracy:0.84 - test accuracy:0.818\n",
      "epochs 181 - train accuracy:0.88 - test accuracy:0.849\n",
      "epochs 191 - train accuracy:0.83 - test accuracy:0.846\n",
      "epochs 201 - train accuracy:0.83 - test accuracy:0.855\n",
      "epochs 211 - train accuracy:0.82 - test accuracy:0.852\n",
      "epochs 221 - train accuracy:0.86 - test accuracy:0.867\n",
      "epochs 231 - train accuracy:0.89 - test accuracy:0.858\n",
      "epochs 241 - train accuracy:0.85 - test accuracy:0.865\n",
      "epochs 251 - train accuracy:0.89 - test accuracy:0.872\n",
      "epochs 261 - train accuracy:0.93 - test accuracy:0.873\n",
      "epochs 271 - train accuracy:0.91 - test accuracy:0.877\n",
      "epochs 281 - train accuracy:0.89 - test accuracy:0.881\n",
      "epochs 291 - train accuracy:0.93 - test accuracy:0.882\n",
      "epochs 301 - train accuracy:0.87 - test accuracy:0.879\n",
      "epochs 311 - train accuracy:0.89 - test accuracy:0.884\n",
      "epochs 321 - train accuracy:0.84 - test accuracy:0.884\n",
      "epochs 331 - train accuracy:0.92 - test accuracy:0.889\n",
      "epochs 341 - train accuracy:0.9 - test accuracy:0.889\n",
      "epochs 351 - train accuracy:0.88 - test accuracy:0.882\n",
      "epochs 361 - train accuracy:0.91 - test accuracy:0.891\n",
      "epochs 371 - train accuracy:0.91 - test accuracy:0.892\n",
      "epochs 381 - train accuracy:0.92 - test accuracy:0.893\n",
      "epochs 391 - train accuracy:0.93 - test accuracy:0.892\n",
      "epochs 401 - train accuracy:0.94 - test accuracy:0.892\n",
      "epochs 411 - train accuracy:0.9 - test accuracy:0.894\n",
      "epochs 421 - train accuracy:0.91 - test accuracy:0.897\n",
      "epochs 431 - train accuracy:0.93 - test accuracy:0.899\n",
      "epochs 441 - train accuracy:0.91 - test accuracy:0.899\n",
      "epochs 451 - train accuracy:0.89 - test accuracy:0.895\n",
      "epochs 461 - train accuracy:0.92 - test accuracy:0.901\n",
      "epochs 471 - train accuracy:0.9 - test accuracy:0.896\n",
      "epochs 481 - train accuracy:0.91 - test accuracy:0.898\n",
      "epochs 491 - train accuracy:0.87 - test accuracy:0.895\n",
      "epochs 501 - train accuracy:0.92 - test accuracy:0.903\n",
      "epochs 511 - train accuracy:0.89 - test accuracy:0.9\n",
      "epochs 521 - train accuracy:0.96 - test accuracy:0.904\n",
      "epochs 531 - train accuracy:0.93 - test accuracy:0.904\n",
      "epochs 541 - train accuracy:0.9 - test accuracy:0.906\n",
      "epochs 551 - train accuracy:0.86 - test accuracy:0.903\n",
      "epochs 561 - train accuracy:0.91 - test accuracy:0.906\n",
      "epochs 571 - train accuracy:0.96 - test accuracy:0.904\n",
      "epochs 581 - train accuracy:0.97 - test accuracy:0.906\n",
      "epochs 591 - train accuracy:0.95 - test accuracy:0.909\n",
      "epochs 601 - train accuracy:0.93 - test accuracy:0.91\n",
      "epochs 611 - train accuracy:0.93 - test accuracy:0.906\n",
      "epochs 621 - train accuracy:0.93 - test accuracy:0.911\n",
      "epochs 631 - train accuracy:0.98 - test accuracy:0.91\n",
      "epochs 641 - train accuracy:0.9 - test accuracy:0.908\n",
      "epochs 651 - train accuracy:0.88 - test accuracy:0.909\n",
      "epochs 661 - train accuracy:0.92 - test accuracy:0.91\n",
      "epochs 671 - train accuracy:0.95 - test accuracy:0.911\n",
      "epochs 681 - train accuracy:0.94 - test accuracy:0.909\n",
      "epochs 691 - train accuracy:0.94 - test accuracy:0.913\n",
      "epochs 701 - train accuracy:0.93 - test accuracy:0.912\n",
      "epochs 711 - train accuracy:0.89 - test accuracy:0.906\n",
      "epochs 721 - train accuracy:0.94 - test accuracy:0.912\n",
      "epochs 731 - train accuracy:0.92 - test accuracy:0.913\n",
      "epochs 741 - train accuracy:0.92 - test accuracy:0.912\n",
      "epochs 751 - train accuracy:0.96 - test accuracy:0.914\n",
      "epochs 761 - train accuracy:0.93 - test accuracy:0.915\n",
      "epochs 771 - train accuracy:0.95 - test accuracy:0.914\n",
      "epochs 781 - train accuracy:0.97 - test accuracy:0.915\n",
      "epochs 791 - train accuracy:0.93 - test accuracy:0.914\n",
      "epochs 801 - train accuracy:0.91 - test accuracy:0.916\n",
      "epochs 811 - train accuracy:0.97 - test accuracy:0.913\n",
      "epochs 821 - train accuracy:0.94 - test accuracy:0.914\n",
      "epochs 831 - train accuracy:0.92 - test accuracy:0.916\n",
      "epochs 841 - train accuracy:0.97 - test accuracy:0.918\n",
      "epochs 851 - train accuracy:0.88 - test accuracy:0.91\n",
      "epochs 861 - train accuracy:0.89 - test accuracy:0.918\n",
      "epochs 871 - train accuracy:0.95 - test accuracy:0.919\n",
      "epochs 881 - train accuracy:0.96 - test accuracy:0.92\n",
      "epochs 891 - train accuracy:0.9 - test accuracy:0.919\n",
      "epochs 901 - train accuracy:0.94 - test accuracy:0.92\n",
      "epochs 911 - train accuracy:0.94 - test accuracy:0.92\n",
      "epochs 921 - train accuracy:0.92 - test accuracy:0.917\n",
      "epochs 931 - train accuracy:0.96 - test accuracy:0.921\n",
      "epochs 941 - train accuracy:0.91 - test accuracy:0.921\n",
      "epochs 951 - train accuracy:0.91 - test accuracy:0.917\n",
      "epochs 961 - train accuracy:0.95 - test accuracy:0.914\n",
      "epochs 971 - train accuracy:0.91 - test accuracy:0.919\n",
      "epochs 981 - train accuracy:0.95 - test accuracy:0.92\n",
      "epochs 991 - train accuracy:0.93 - test accuracy:0.924\n"
     ]
    }
   ],
   "source": [
    "# 2-Layer Neural Network Design\n",
    "\n",
    "train_size, input_size = X_train.shape\n",
    "hidden_size = 50\n",
    "_, output_size = y_train.shape\n",
    "\n",
    "steps = 1000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "network = TwoLayerNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "for i in range(steps):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # Back propagation\n",
    "    grads = network.gradient(X_batch, y_batch)\n",
    "    \n",
    "    # SGD\n",
    "    for key in network.params.keys():\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "        \n",
    "    # Parameter 갱신 후, get loss\n",
    "    loss = network.loss(X_batch, y_batch)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        tr_acc = network.accuracy(X_batch, y_batch)\n",
    "        te_acc = network.accuracy(X_test, y_test)\n",
    "        \n",
    "        train_acc.append(tr_acc)\n",
    "        test_acc.append(te_acc)\n",
    "        \n",
    "        print(\"epochs {} - train accuracy:{} - test accuracy:{}\".format(i + 1, round(tr_acc, 3), round(te_acc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2d86a",
   "metadata": {},
   "source": [
    "## 번외. 검증에 활용되는 수치 미분\n",
    "- 수치 미분은 상대적으로 오차역전파보다 오래걸리는 특징을 가진다.\n",
    "- 하지만 직접 수학적 계산을 했기 때문에 해석적 방법을 사용하는 오차역전파 결과를 검증하는데에 사용한다.\n",
    "    - 기울기 확인 (Gradient Check)\n",
    "- 두 값의 차이가 0에 가깝다면 오차역전파를 통한 기울기 계산이 잘 되었다고 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40840c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:W1, diff:4.70453984756419e-10\n",
      "key:b1, diff:2.405896911010009e-09\n",
      "key:W2, diff:5.9518257291758695e-09\n",
      "key:b2, diff:1.398746876560275e-07\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=input_size, hidden_size=hidden_size, output_size=10)\n",
    "\n",
    "# Batch\n",
    "X_batch = X_train[:3]\n",
    "y_batch = y_train[:3]\n",
    "\n",
    "# 수치미분\n",
    "grad_numerical = network.numerical_gradient(X_batch, y_batch)\n",
    "grad_propagation = network.gradient(X_batch, y_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.mean(np.abs(grad_numerical[key] - grad_propagation[key]))\n",
    "    print('key:{}, diff:{}'.format(key, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50c17348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.470453984756419"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.70453984756419e-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
